# TOKENIZERs
# Letter tokenizer - divide la cadena quedándose únicamente con las letras.
GET _analyze?tokenizer=letter&text=esto123213es<<<>>una$·prueba

# Lowercase tokenizer - letter tokenizer que transforma el texto a lowercase.
GET _analyze?tokenizer=lowercase&text=ESTO123213ES<<<>>una$·prueba

# Whitespace tokenizer - genera tokens de texto separado por espacios en blanco.
GET _analyze?tokenizer=whitespace&text=esto es una prueba

# Path_hierarchy - muy útil para tokenizar cadenas de tipo path
GET _analyze?tokenizer=uax_url_email
{
	hey, check out http://edge.org/conversation/yuval_noah_harari-daniel_kahneman-death-is-optional for some light reading.
}

GET _analyze?tokenizer=uax_url_email
{
    luis.polanco@gmail.com, torpedo, juan.alvarez@hotmail.com
}

# nGram - trocea el contenido en bloques de min_gram y max_gram
DELETE indice
PUT indice
{
    "settings": {
        "analysis": {
            "analyzer": {
                "un_ngram_tokenizer_cualquiera": {
                    "tokenizer": "un_ngram_tokenizer_cualquiera"
                }
            },
            "tokenizer": {
                "un_ngram_tokenizer_cualquiera": {
                    "type": "nGram",
                    "min_gram": "3",
                    "max_gram": "4",
                    "token_chars": [
                        "letter",
                        "digit"
                    ]
                }
            }
        }
    }
}
GET indice/_analyze?pretty=1&analyzer=un_ngram_tokenizer_cualquiera&text=simple

